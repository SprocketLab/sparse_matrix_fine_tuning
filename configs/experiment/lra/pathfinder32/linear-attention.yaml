# @package _global_

# to execute this experiment run:
# python run.py experiment=example_simple.yaml

defaults:
  - /experiment/lra/pathfinder32/transformer
  - override /model/attn_cfg: linear-attention
