# @package _global_
defaults:
  - /experiment/lra/pathfinder32/transformer
  - override /model/attn_cfg: combination

model:
  attn_cfg:
    attn_cfg_0:  # Smyrf
      attention_dropout: 0.2
      q_cluster_size: 16
      k_cluster_size: ${.q_cluster_size}
      n_hashes: 2
    attn_cfg_1:  # Performer
      nb_features: 16
      softmax_eps: 1e-4
