# @package _global_

# to execute this experiment run:
# python run.py experiment=example_simple.yaml

# We use the hyperparameters from https://github.com/google-research/long-range-arena/issues/16

defaults:
  - override /trainer: default # choose trainer from 'configs/trainer/'
  - override /model: transformer-classifier
  - override /model/attn_cfg: full
  - override /datamodule: lra-imagec
  - override /scheduler: cosine-warmup
  - override /callbacks: default.yaml
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

model:
  d_model: 128
  n_head: 8
  n_layer: 1
  d_inner: 128
  dropout: 0.3
  mha_cfg:
    kdim: 64
    vdim: ${.kdim}
  pos_encoding_cfg:
    # max_len: 1024
    max_len: ${datamodule.__max_len}
    initializer:  # learnable positional encoding
      _target_: torch.nn.init.normal_
      std: 0.02
  pooling_mode: CLS

seed: 1111

trainer:
  accelerator: gpu
  devices: 1
  max_epochs: 200
  accumulate_grad_batches: ${div_up:${train.global_batch_size}, ${eval:${trainer.devices} * ${datamodule.batch_size}}}

train:
  global_batch_size: 256
  optimizer:
    lr: 0.0005
    betas: [0.9, 0.98]
    weight_decay: 0.0
  scheduler:
    num_warmup_steps: ${div_up:${datamodule.__train_len}, ${train.global_batch_size}}
    num_training_steps: ${eval:${div_up:${datamodule.__train_len}, ${train.global_batch_size}} * ${trainer.max_epochs}}

datamodule:
  batch_size: 256
