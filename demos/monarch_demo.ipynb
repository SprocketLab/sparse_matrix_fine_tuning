{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/wenxuan/sparse_matrix_fine_tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/wenxuan/miniconda3/envs/ldm/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# You might wanna connect to docker env using the command below inside docker, \n",
    "# then select jupyter server http://127.0.0.1:5050/lab?token=  (default password is 'local')\n",
    "# python3 -m jupyterlab --no-browser --ip=0.0.0.0 --port=5050 --allow-root --NotebookApp.token='local' --NotebookApp.password='local'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())\n",
    "import torch\n",
    "from src.layers.monarch_linear import MonarchLinear\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([16, 1024])\n",
      "monarch factor shape (nblocks, block rank, block size):  torch.Size([4, 4, 256]) torch.Size([4, 256, 4])\n",
      "Total parameters: 0.009M,\n",
      "         trainable parameters: 0.009M (100.000%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MonarchLinear(in_features=1024, out_features=1024, nblocks=4, requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "from train_utils import param_stats\n",
    "\n",
    "peft_config = json.load(open(\"task_configs/monarch_roberta_glue/peft_config.json\", \"r\"))\n",
    "monarch = MonarchLinear(in_features=1024, out_features=1024, peft_config=peft_config, as_adapter=False)\n",
    "\n",
    "x = torch.randn(16, 1024, device=\"cuda\")\n",
    "print(\"out.shape:\", monarch(x).shape)\n",
    "print(\"monarch factor shape (nblocks, block rank, block size): \", monarch.blkdiag1.shape, monarch.blkdiag2.shape)\n",
    "param_stats(monarch)\n",
    "monarch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting layer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of exact layers to adapt with Monarch:  ['query', 'value', 'key']\n",
      "Can also modify the below options:\n",
      "adapt querys and keys only:  False\n",
      "adapt mlp: False\n",
      "making block rank = block size: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'monarch': True,\n",
       " 'square': False,\n",
       " 'nblocks': 4,\n",
       " 'blk_r': 4,\n",
       " 'blk_sz': None,\n",
       " 'target_modules': ['query', 'value', 'key'],\n",
       " 'q_v': False,\n",
       " 'adapter': True,\n",
       " 'scaler': False,\n",
       " 'layernorm': True,\n",
       " 'large_lr': False,\n",
       " 'new_lr': 0.005,\n",
       " 'scaler_type': 'scaler',\n",
       " 'from_lora': '',\n",
       " 'mlp': False,\n",
       " 'lora_style_init': False,\n",
       " 'use_mult_factor': False,\n",
       " 'affine': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Names of exact layers to adapt with Monarch: \", peft_config[\"target_modules\"])\n",
    "print(\"Can also modify the below options:\")\n",
    "print(\"adapt querys and keys only: \", peft_config[\"q_v\"])\n",
    "print(\"adapt mlp:\", peft_config[\"mlp\"])\n",
    "print(\"making block rank = block size:\", peft_config[\"square\"])\n",
    "# Can safely ignore other settings\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monarch factor shape: torch.Size([2, 8, 512]) torch.Size([2, 512, 8])\n"
     ]
    }
   ],
   "source": [
    "peft_config[\"blk_r\"] = 8\n",
    "peft_config[\"blk_sz\"] = 512\n",
    "monarch = MonarchLinear(in_features=1024, out_features=1024, peft_config=peft_config)\n",
    "print(\"monarch factor shape:\", monarch.blkdiag1.shape, monarch.blkdiag2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense matrix approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 192]) torch.Size([4, 192, 32]) torch.Size([4, 736, 192]) torch.Size([4, 192, 736])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.7431, -0.2139, -2.8119,  ..., -0.8477, -1.4322,  0.1578],\n",
       "         [-1.3386, -0.3959,  0.5944,  ..., -0.1258, -0.6600, -0.4448],\n",
       "         [-1.7184, -0.6263, -0.5038,  ...,  1.3683,  0.0636, -0.1049],\n",
       "         ...,\n",
       "         [ 1.6679,  0.8312,  0.0915,  ..., -0.5639, -0.1153,  0.7217],\n",
       "         [ 1.5410, -0.8582,  0.1301,  ..., -1.2981, -0.1931,  0.7246],\n",
       "         [ 0.8562, -1.1198, -0.3811,  ..., -0.1794, -0.0373,  1.6991]],\n",
       "        device='cuda:0'),\n",
       " tensor([[-0.7432, -0.2139, -2.8120,  ..., -0.8478, -1.4323,  0.1578],\n",
       "         [-1.3386, -0.3960,  0.5943,  ..., -0.1257, -0.6601, -0.4448],\n",
       "         [-1.7184, -0.6263, -0.5038,  ...,  1.3683,  0.0636, -0.1050],\n",
       "         ...,\n",
       "         [ 1.6680,  0.8312,  0.0915,  ..., -0.5639, -0.1153,  0.7217],\n",
       "         [ 1.5410, -0.8582,  0.1301,  ..., -1.2982, -0.1931,  0.7247],\n",
       "         [ 0.8563, -1.1199, -0.3812,  ..., -0.1794, -0.0373,  1.6991]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project to two monarch factors using SVD.\n",
    "from src.ops.blockdiag_butterfly_einsum import blockdiag_butterfly_project_einsum_rank, blockdiag_butterfly_multiply_einsum_rank\n",
    "from src.ops.blockdiag_butterfly_multiply import blockdiag_butterfly_multiply\n",
    "torch.random.manual_seed(0)\n",
    "dim = 768\n",
    "weights = torch.randn(dim, dim, device=\"cuda\")\n",
    "nblocks = 4\n",
    "rank = 8 \n",
    "i = torch.eye(dim, device=\"cuda\")\n",
    "blkdiag1, blkdiag2, rev1, rev2 = blockdiag_butterfly_project_einsum_rank(weights.T, nblocks, nblocks, rank, reverse=True)\n",
    "print(blkdiag1.shape, blkdiag2.shape, rev1.shape, rev2.shape)\n",
    "(weights - blockdiag_butterfly_multiply(i, blkdiag1, blkdiag2)), blockdiag_butterfly_multiply(i, rev1, rev2)\n",
    "\n",
    "# We don't use it for our setup, but if curious check blockdiag_butterfly_project_einsum_rank\n",
    "# from torch.testing import assert_allclose\n",
    "# from copy import deepcopy\n",
    "# m, n = 1024, 512\n",
    "# weights = torch.randn(m, n, device=\"cuda\")\n",
    "# monarch = MonarchLinear(in_features=n, out_features=m, weights=weights, peft_config=peft_config)\n",
    "# x = torch.eye(n, device=\"cuda\")\n",
    "# assert_allclose(monarch(x),  x @ weights.T )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0965, -1.2275, -0.1063,  ..., -0.1150,  0.0686,  0.6476],\n",
       "         [-0.0931,  0.2505, -0.5180,  ...,  0.6185,  0.9747, -0.2307],\n",
       "         [-0.0970,  0.0775,  0.7878,  ...,  1.5981, -0.3175,  1.6818],\n",
       "         ...,\n",
       "         [-1.1939,  0.2523,  1.0238,  ..., -1.1577,  0.3693, -0.7117],\n",
       "         [ 1.0135, -1.5772,  1.6735,  ...,  0.0610, -0.1385, -0.3062],\n",
       "         [ 0.7105, -0.2479,  0.6746,  ..., -0.1854,  0.4084, -0.5091]]),\n",
       " tensor([[-1.0965, -1.2275, -0.1063,  ..., -0.1150,  0.0686,  0.6476],\n",
       "         [-0.0931,  0.2505, -0.5180,  ...,  0.6185,  0.9747, -0.2307],\n",
       "         [-0.0970,  0.0775,  0.7878,  ...,  1.5981, -0.3175,  1.6818],\n",
       "         ...,\n",
       "         [-1.1939,  0.2523,  1.0238,  ..., -1.1577,  0.3693, -0.7117],\n",
       "         [ 1.0135, -1.5772,  1.6735,  ...,  0.0610, -0.1385, -0.3062],\n",
       "         [ 0.7105, -0.2479,  0.6746,  ..., -0.1854,  0.4084, -0.5091]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.randn(100, 100)\n",
    "U, S, Vt = torch.linalg.svd(rand)\n",
    "rank = 10\n",
    "principal = U[:, :rank] @ torch.diag(S[:rank]) @ Vt[:rank, :]\n",
    "least = U[:, rank:] @ torch.diag(S[rank:]) @ Vt[rank:, :]\n",
    "rand - principal, least\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora-style model adaptation for (theoretically) any model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 338.897M,\n",
      "         trainable parameters: 338.897M (100.000%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "355359744"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    "    AutoModel\n",
    ")\n",
    "model_name = \"roberta-large\"\n",
    "# model_name = \"meta-llama/Llama-2-7b\"  # This one requies api key\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "param_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up for the specific layer names to adapt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapted query (1024, 1024) with monarch layers: torch.Size([4, 4, 256]), torch.Size([4, 256, 4])\n",
      "Adapted key (1024, 1024) with monarch layers: torch.Size([4, 4, 256]), torch.Size([4, 256, 4])\n",
      "Adapted value (1024, 1024) with monarch layers: torch.Size([4, 4, 256]), torch.Size([4, 256, 4])\n",
      "Total parameters: 339.460M,\n",
      "         trainable parameters: 0.633M (0.186%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "663552"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_utils import init_monarch_layers\n",
    "\n",
    "peft_config = json.load(open(\"task_configs/monarch_roberta_glue/peft_config.json\", \"r\"))\n",
    "peft_config['target_modules'] = [\"query\", \"key\", \"value\"]\n",
    "init_monarch_layers(model, peft_config)\n",
    "param_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wanna see what layers are adapted? 🥹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.0.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.0.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.0.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.0.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.0.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.0.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.0.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.0.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.0.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.1.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.1.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.1.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.1.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.1.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.1.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.1.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.1.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.1.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.2.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.2.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.2.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.2.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.2.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.2.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.2.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.2.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.2.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.3.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.3.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.3.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.3.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.3.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.3.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.3.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.3.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.3.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.4.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.4.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.4.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.4.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.4.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.4.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.4.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.4.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.4.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.5.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.5.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.5.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.5.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.5.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.5.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.5.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.5.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.5.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.6.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.6.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.6.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.6.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.6.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.6.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.6.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.6.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.6.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.7.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.7.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.7.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.7.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.7.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.7.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.7.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.7.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.7.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.8.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.8.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.8.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.8.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.8.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.8.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.8.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.8.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.8.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.9.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.9.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.9.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.9.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.9.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.9.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.9.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.9.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.9.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.10.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.10.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.10.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.10.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.10.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.10.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.10.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.10.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.10.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.11.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.11.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.11.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.11.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.11.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.11.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.11.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.11.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.11.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.12.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.12.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.12.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.12.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.12.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.12.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.12.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.12.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.12.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.13.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.13.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.13.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.13.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.13.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.13.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.13.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.13.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.13.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.14.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.14.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.14.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.14.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.14.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.14.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.14.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.14.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.14.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.15.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.15.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.15.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.15.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.15.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.15.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.15.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.15.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.15.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.16.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.16.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.16.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.16.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.16.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.16.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.16.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.16.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.16.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.17.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.17.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.17.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.17.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.17.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.17.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.17.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.17.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.17.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.18.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.18.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.18.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.18.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.18.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.18.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.18.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.18.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.18.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.19.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.19.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.19.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.19.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.19.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.19.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.19.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.19.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.19.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.20.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.20.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.20.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.20.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.20.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.20.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.20.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.20.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.20.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.21.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.21.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.21.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.21.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.21.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.21.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.21.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.21.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.21.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.22.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.22.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.22.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.22.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.22.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.22.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.22.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.22.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.22.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.23.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.23.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.23.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.23.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.23.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.23.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.23.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.23.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.23.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "Total parameters: 339.460M,\n",
      "         trainable parameters: 0.633M (0.186%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "663552"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_stats(model, print_trainable=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
