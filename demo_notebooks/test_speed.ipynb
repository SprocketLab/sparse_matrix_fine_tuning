{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e61d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65a46b",
   "metadata": {},
   "source": [
    "__Baseline:__ pytorch linear with a full dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3ca6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = nn.Sequential(nn.Linear(4096, 16384), nn.ReLU(), nn.Linear(16384, 4096)).to(device=device)\n",
    "baseline_size = sum(p.numel() for p in baseline.parameters())\n",
    "input = torch.randn(16, 512, 4096, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0265fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.43 s, sys: 4.08 s, total: 5.52 s\n",
      "Wall time: 5.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    out = baseline(input)\n",
    "    out.sum().backward()\n",
    "    \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edc898db",
   "metadata": {},
   "outputs": [],
   "source": [
    "del baseline, out\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b19be3f",
   "metadata": {},
   "source": [
    "## Same, with Vanilla pytorch Monarch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26286327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "class MonarchLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int,\n",
    "                 in_dims: Sequence[int], out_dims: Sequence[int],\n",
    "                 bias: bool = True, checkpoint: bool = False,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Monarch linear layer, a generalization of https://arxiv.org/abs/2204.00595\n",
    "\n",
    "        Ths implementation interprets Monarch as a product over an M by M grid (in_features=M ^ 2).\n",
    "        The first product applies over all rows of the grid, the second runs over columns.\n",
    "        In general, the grid may have uneven size or more than 2 dimensions.\n",
    "\n",
    "        In the 2d case, the two products use [M x M x M] weight tensors. In the general case,\n",
    "        it uses grid_dim weight tensors of shape [grid_numel / in_dims[i], in_dims[i], out_dims[i]].\n",
    "\n",
    "        :param in_features: input dimension, same as in nn.Linear\n",
    "        :param out_features: output dimension, same as in nn.Linear\n",
    "        :param in_dims: a tuple of numbers that multiply to in_features, see example below\n",
    "        :param out_dims: a tuple of numbers that multiply to out_features, see example below\n",
    "        :param bias: whether or not to use a bias term, same as in nn.Linear\n",
    "        :param checkpoint: if True, apply gradient checkpointing over this entire layer.\n",
    "           This adds ~30% compute overhead for forward+backward, but reduces the memory overhead;\n",
    "           otherwise, monarch must to store ndim - 1 additional tensors for intermediate activations.\n",
    "\n",
    "        :example:\n",
    "\n",
    "        >>> # classic monarch:\n",
    "        >>> MonarchLinear(in_features=1024, in_dims=(32, 32), out_features=1024, out_dims=(32, 32))\n",
    "        >>> # generalization to rectangular matrices\n",
    "        >>> MonarchLinear(in_features=1024, in_dims=(32, 32), out_features=4096, out_dims=(64, 64))\n",
    "        >>> MonarchLinear(in_features=1024, in_dims=(32, 32), out_features=1536, out_dims=(32, 48))\n",
    "        >>> # generalization to higher dimension\n",
    "        >>> MonarchLinear(in_features=4096, in_dims=(16, 16, 16), out_features=4096, out_dims=(16, 16, 16))\n",
    "        >>> MonarchLinear(in_features=4096, in_dims=(16, 16, 16), out_features=1536, out_dims=(8, 12, 16))\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(in_dims) == len(out_dims) and len(in_dims) > 1\n",
    "        assert np.prod(in_dims) == in_features\n",
    "        assert np.prod(out_dims) == out_features\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "        self.in_dims, self.out_dims = in_dims, out_dims\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "        # construct weight tensors by keeping track of intermediate tensor dimension at each step\n",
    "        self.weights = nn.ParameterList()\n",
    "        current_numel = np.prod(in_dims)\n",
    "        assert current_numel == in_features\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(in_dims, out_dims)):\n",
    "            self.weights.append(nn.Parameter(torch.empty(current_numel // in_dim, in_dim, out_dim)))\n",
    "            current_numel = current_numel // in_dim * out_dim\n",
    "        assert current_numel == out_features\n",
    "        self.register_parameter('bias', nn.Parameter(torch.empty(out_features)) if bias else None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self, gain: float = 1.0):\n",
    "        # initialize, re-scale to account for the number of multiplied tensors\n",
    "        init_std = (gain / np.sqrt(self.in_features)) ** (1 / len(self.in_dims))\n",
    "        for weight in self.weights:\n",
    "            nn.init.normal_(weight, std=init_std)\n",
    "        if self.bias is not None:\n",
    "            bound = 1 / np.sqrt(self.in_features)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, _inside_checkpoint: bool = False):\n",
    "        if self.checkpoint and not _inside_checkpoint and torch.is_grad_enabled():\n",
    "            return checkpoint(partial(self.forward, _inside_checkpoint=True),\n",
    "                              input if input.requires_grad else input.detach().requires_grad_(True),\n",
    "                              preserve_rng_state=False)\n",
    "        input_shape = input.shape\n",
    "        tensor = input.view(-1, *self.in_dims)\n",
    "        # shape: [flat_batch_size, in_dim[0], ..., in_dim[N]]\n",
    "\n",
    "        del input\n",
    "        tensor = tensor.permute(*np.roll(range(len(self.in_dims) + 1), -2))\n",
    "        # new shape: [in_dim[1], ..., in_dim[N - 1], flat_batch_size, in_dim[0]]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # loop maintains tensor in the following shape: [*all_dims_except_i, batch, dim[i]]\n",
    "\n",
    "            tensor = torch.bmm(\n",
    "                tensor.flatten(0, -3), self.weights[i]\n",
    "            ).view(*tensor.shape[:-1], -1)\n",
    "            # ^-- BMM, output: [*other_dims, batch, out_dim[i]]\n",
    "            #     left input:  [*other_dims, batch, in_dim[i]]\n",
    "            #     right_input: [*other_dims, in_dim[i], out_dim[i]]\n",
    "\n",
    "            # prepare next step, from [*other_dims, batch, out_dim[i]] to [*other_dims, batch, in_dim[i + 1]]\n",
    "            tensor = tensor.swapaxes_(-1, i)\n",
    "            # note: we can swap in-place because bmm does not need outputs for backprop\n",
    "\n",
    "        # after loop: [out_dim[0], ..., out_dim[N - 1], batch]\n",
    "        tensor = tensor.flatten(0, -2).swapaxes_(0, 1)\n",
    "        tensor = tensor.reshape(*input_shape[:-1], -1)\n",
    "        if self.bias is not None:\n",
    "            tensor += self.bias\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b41f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 64, 64]), torch.Size([64, 64, 256]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monarch = nn.Sequential(\n",
    "    MonarchLinear(4096, 16384, in_dims=(64, 64), out_dims=(64, 256)),\n",
    "    nn.ReLU(),\n",
    "    MonarchLinear(16384, 4096, in_dims=(256, 64), out_dims=(64, 64))\n",
    ").to(device)\n",
    "monarch[0].weights[0].shape, monarch[0].weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8abb1a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.42 s, sys: 2.61 s, total: 4.03 s\n",
      "Wall time: 4.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    out = monarch(input)\n",
    "    out.sum().backward()\n",
    "    \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9c34be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del monarch, out\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25502a9d",
   "metadata": {},
   "source": [
    "## Test my monarch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a8663ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-12 07:37:00,290] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "None\n",
      "in_features: 4096, nblocks: 4\n",
      "None\n",
      "in_features: 16384, nblocks: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkProcess-2:\n",
      "Process ForkProcess-1:\n",
      "Process ForkProcess-3:\n",
      "Process ForkProcess-8:\n",
      "Process ForkProcess-6:\n",
      "Process ForkProcess-5:\n",
      "Process ForkProcess-4:\n",
      "Process ForkProcess-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/concurrent/futures/process.py\", line 240, in _process_worker\n",
      "    call_item = call_queue.get(block=True)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 103, in get\n",
      "    res = self._recv_bytes()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 102, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "from src.models.layers.monarch_linear import MonarchLinear as MyMonarch\n",
    "monarch = nn.Sequential(\n",
    "    MyMonarch(4096, 16384, as_adapter=False),\n",
    "    nn.ReLU(),\n",
    "    MyMonarch(16384, 4096, as_adapter=False)\n",
    ").to(device)\n",
    "monarch[0].as_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5506ebe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 535 ms, sys: 620 ms, total: 1.15 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    out = monarch(input)\n",
    "    out.sum().backward()\n",
    "    \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffb67dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression rate (parameters): 0.0017392365666859914\n"
     ]
    }
   ],
   "source": [
    "monarch_size = sum(p.numel() for p in monarch.parameters())\n",
    "print(\"Compression rate (parameters):\", monarch_size / baseline_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68051a51",
   "metadata": {},
   "source": [
    "__Generalized 3D Monarch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a168c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "monarch = nn.Sequential(\n",
    "    MonarchLinear(4096, 16384, in_dims=(16, 16, 16), out_dims=(16, 16, 64)),\n",
    "    nn.ReLU(),\n",
    "    MonarchLinear(16384, 4096, in_dims=(64, 16, 16), out_dims=(16, 16, 16))\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35b3fa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 s, sys: 2.11 s, total: 3.5 s\n",
      "Wall time: 3.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    out = monarch(input)\n",
    "    out.sum().backward()\n",
    "    \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b303548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression rate (parameters): 0.006011045677844567\n"
     ]
    }
   ],
   "source": [
    "monarch_size = sum(p.numel() for p in monarch.parameters())\n",
    "print(\"Compression rate (parameters):\", monarch_size / baseline_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
