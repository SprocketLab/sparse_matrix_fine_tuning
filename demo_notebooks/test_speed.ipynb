{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48e61d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA A100-PCIE-40GB\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeab5d1",
   "metadata": {},
   "source": [
    "### Contiguous vs non-contiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298d5b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.93 s ± 32.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(10000, 10000, dtype=torch.float32, device=device)\n",
    "w = torch.ones(10000, 10, dtype=torch.float32, device=device)\n",
    "# Compare contiguous vs non-contiguous\n",
    "def test(a, w):\n",
    "    for _ in range(10000):\n",
    "        x = a @ w\n",
    "%timeit test(a, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e52c7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.88 s ± 32.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "a = a.transpose(0, 1)\n",
    "%timeit test(a, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65a46b",
   "metadata": {},
   "source": [
    "__Baseline:__ pytorch linear with a full dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3ca6eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = nn.Sequential(nn.Linear(4096, 16384), nn.ReLU(), nn.Linear(16384, 4096)).to(device=device)\n",
    "baseline_size = sum(p.numel() for p in baseline.parameters())\n",
    "input = torch.randn(16, 512, 4096, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0265fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.68 s, sys: 1.6 s, total: 5.28 s\n",
      "Wall time: 5.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    out = baseline(input)\n",
    "    out.sum().backward()\n",
    "    \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edc898db",
   "metadata": {},
   "outputs": [],
   "source": [
    "del baseline, out\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b19be3f",
   "metadata": {},
   "source": [
    "## Same, with Vanilla pytorch Monarch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26286327",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from typing import Sequence\n",
    "\n",
    "\n",
    "class MonarchLinear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int,\n",
    "                 in_dims: Sequence[int], out_dims: Sequence[int],\n",
    "                 bias: bool = True, checkpoint: bool = False,\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Monarch linear layer, a generalization of https://arxiv.org/abs/2204.00595\n",
    "\n",
    "        Ths implementation interprets Monarch as a product over an M by M grid (in_features=M ^ 2).\n",
    "        The first product applies over all rows of the grid, the second runs over columns.\n",
    "        In general, the grid may have uneven size or more than 2 dimensions.\n",
    "\n",
    "        In the 2d case, the two products use [M x M x M] weight tensors. In the general case,\n",
    "        it uses grid_dim weight tensors of shape [grid_numel / in_dims[i], in_dims[i], out_dims[i]].\n",
    "\n",
    "        :param in_features: input dimension, same as in nn.Linear\n",
    "        :param out_features: output dimension, same as in nn.Linear\n",
    "        :param in_dims: a tuple of numbers that multiply to in_features, see example below\n",
    "        :param out_dims: a tuple of numbers that multiply to out_features, see example below\n",
    "        :param bias: whether or not to use a bias term, same as in nn.Linear\n",
    "        :param checkpoint: if True, apply gradient checkpointing over this entire layer.\n",
    "           This adds ~30% compute overhead for forward+backward, but reduces the memory overhead;\n",
    "           otherwise, monarch must to store ndim - 1 additional tensors for intermediate activations.\n",
    "\n",
    "        :example:\n",
    "\n",
    "        >>> # classic monarch:\n",
    "        >>> MonarchLinear(in_features=1024, in_dims=(32, 32), out_features=1024, out_dims=(32, 32))\n",
    "        >>> # generalization to rectangular matrices\n",
    "        >>> MonarchLinear(in_features=1024, in_dims=(32, 32), out_features=4096, out_dims=(64, 64))\n",
    "        >>> MonarchLinear(in_features=1024, in_dims=(32, 32), out_features=1536, out_dims=(32, 48))\n",
    "        >>> # generalization to higher dimension\n",
    "        >>> MonarchLinear(in_features=4096, in_dims=(16, 16, 16), out_features=4096, out_dims=(16, 16, 16))\n",
    "        >>> MonarchLinear(in_features=4096, in_dims=(16, 16, 16), out_features=1536, out_dims=(8, 12, 16))\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert len(in_dims) == len(out_dims) and len(in_dims) > 1\n",
    "        assert np.prod(in_dims) == in_features\n",
    "        assert np.prod(out_dims) == out_features\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "        self.in_dims, self.out_dims = in_dims, out_dims\n",
    "        self.checkpoint = checkpoint\n",
    "\n",
    "        # construct weight tensors by keeping track of intermediate tensor dimension at each step\n",
    "        self.weights = nn.ParameterList()\n",
    "        current_numel = np.prod(in_dims)\n",
    "        assert current_numel == in_features\n",
    "        for i, (in_dim, out_dim) in enumerate(zip(in_dims, out_dims)):\n",
    "            self.weights.append(nn.Parameter(torch.empty(current_numel // in_dim, in_dim, out_dim)))\n",
    "            current_numel = current_numel // in_dim * out_dim\n",
    "        assert current_numel == out_features\n",
    "        self.register_parameter('bias', nn.Parameter(torch.empty(out_features)) if bias else None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self, gain: float = 1.0):\n",
    "        # initialize, re-scale to account for the number of multiplied tensors\n",
    "        init_std = (gain / np.sqrt(self.in_features)) ** (1 / len(self.in_dims))\n",
    "        for weight in self.weights:\n",
    "            nn.init.normal_(weight, std=init_std)\n",
    "        if self.bias is not None:\n",
    "            bound = 1 / np.sqrt(self.in_features)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: torch.Tensor, _inside_checkpoint: bool = False):\n",
    "        if self.checkpoint and not _inside_checkpoint and torch.is_grad_enabled():\n",
    "            return checkpoint(partial(self.forward, _inside_checkpoint=True),\n",
    "                              input if input.requires_grad else input.detach().requires_grad_(True),\n",
    "                              preserve_rng_state=False)\n",
    "        input_shape = input.shape\n",
    "        tensor = input.view(-1, *self.in_dims)\n",
    "        # shape: [flat_batch_size, in_dim[0], ..., in_dim[N]]\n",
    "\n",
    "        del input\n",
    "        tensor = tensor.permute(*np.roll(range(len(self.in_dims) + 1), -2))\n",
    "        # new shape: [in_dim[1], ..., in_dim[N - 1], flat_batch_size, in_dim[0]]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            # loop maintains tensor in the following shape: [*all_dims_except_i, batch, dim[i]]\n",
    "\n",
    "            tensor = torch.bmm(\n",
    "                tensor.flatten(0, -3), self.weights[i]\n",
    "            ).view(*tensor.shape[:-1], -1)\n",
    "            # ^-- BMM, output: [*other_dims, batch, out_dim[i]]\n",
    "            #     left input:  [*other_dims, batch, in_dim[i]]\n",
    "            #     right_input: [*other_dims, in_dim[i], out_dim[i]]\n",
    "\n",
    "            # prepare next step, from [*other_dims, batch, out_dim[i]] to [*other_dims, batch, in_dim[i + 1]]\n",
    "            tensor = tensor.swapaxes_(-1, i)\n",
    "            # note: we can swap in-place because bmm does not need outputs for backprop\n",
    "\n",
    "        # after loop: [out_dim[0], ..., out_dim[N - 1], batch]\n",
    "        tensor = tensor.flatten(0, -2).swapaxes_(0, 1)\n",
    "        tensor = tensor.reshape(*input_shape[:-1], -1)\n",
    "        if self.bias is not None:\n",
    "            tensor += self.bias\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67b41f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 64, 64]), torch.Size([64, 64, 256]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monarch = nn.Sequential(\n",
    "    MonarchLinear(4096, 16384, in_dims=(64, 64), out_dims=(64, 256)),\n",
    "    nn.ReLU(),\n",
    "    MonarchLinear(16384, 4096, in_dims=(256, 64), out_dims=(64, 64))\n",
    ").to(device)\n",
    "monarch[0].weights[0].shape, monarch[0].weights[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8abb1a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 921 ms, sys: 427 ms, total: 1.35 s\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    out = monarch(input)\n",
    "    out.sum().backward()\n",
    "    \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d9c34be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del monarch, out\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25502a9d",
   "metadata": {},
   "source": [
    "## Test my monarch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a8663ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 1024])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.layers.monarch_linear import MonarchLinear as MyMonarch\n",
    "nblocks = 4\n",
    "monarch = nn.Sequential(\n",
    "    MyMonarch(4096, 16384, as_adapter=False, nblocks=nblocks),\n",
    "    nn.ReLU(),\n",
    "    MyMonarch(16384, 4096, as_adapter=False, nblocks=nblocks)\n",
    ").to(device)\n",
    "monarch[0].blkdiag1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5506ebe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 820 ms, sys: 314 ms, total: 1.13 s\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    out = monarch(input)\n",
    "    out.sum().backward()\n",
    "    \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffb67dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression rate (parameters): 0.0017392365666859914\n"
     ]
    }
   ],
   "source": [
    "monarch_size = sum(p.numel() for p in monarch.parameters())\n",
    "print(\"Compression rate (parameters):\", monarch_size / baseline_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "383680c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.randn(16, 1024, device=\"cuda\")\n",
    "w = torch.nn.Parameter(torch.randn(1024, 1024, device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4971b3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.2 ms, sys: 14 ms, total: 33.2 ms\n",
      "Wall time: 39.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for i in range(100):\n",
    "    out = torch.matmul(x1, w)\n",
    "    out.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c145e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.nn.Parameter(torch.randn(4, 256, 256, device=\"cuda\"))\n",
    "x = x1.reshape(16, 4, 256).permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cc99e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 ms, sys: 8.02 ms, total: 28.7 ms\n",
      "Wall time: 34.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "for i in range(100):\n",
    "    # x = x1.reshape(16, 4, 256).permute(1, 0, 2)\n",
    "    out = torch.bmm(x, w)\n",
    "    out = out.permute(1, 0, 2).reshape(16, 1024)\n",
    "    out.sum().backward()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68051a51",
   "metadata": {},
   "source": [
    "__Generalized 3D Monarch__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a168c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "monarch = nn.Sequential(\n",
    "    MonarchLinear(4096, 16384, in_dims=(16, 16, 16), out_dims=(16, 16, 64)),\n",
    "    nn.ReLU(),\n",
    "    MonarchLinear(16384, 4096, in_dims=(64, 16, 16), out_dims=(16, 16, 16))\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35b3fa94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.96 s, sys: 1.52 s, total: 3.48 s\n",
      "Wall time: 3.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(100):\n",
    "    out = monarch(input)\n",
    "    out.sum().backward()\n",
    "    \n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b303548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression rate (parameters): 0.006011045677844567\n"
     ]
    }
   ],
   "source": [
    "monarch_size = sum(p.numel() for p in monarch.parameters())\n",
    "print(\"Compression rate (parameters):\", monarch_size / baseline_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
