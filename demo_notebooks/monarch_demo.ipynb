{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tests.ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest_blockdiag_butterfly_projection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test_trained_weight_approx\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmonarch_linear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MonarchLinear\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tests.ops'"
     ]
    }
   ],
   "source": [
    "# You might wanna connect to docker env using the command below inside docker, \n",
    "# then select jupyter server http://127.0.0.1:5050/lab?token=  (default password is 'local')\n",
    "# python3 -m jupyterlab --no-browser --ip=0.0.0.0 --port=5050 --allow-root --NotebookApp.token='local' --NotebookApp.password='local'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import os\n",
    "print(os.getcwd())\n",
    "import torch\n",
    "from tests.ops.test_blockdiag_butterfly_projection import test_trained_weight_approx\n",
    "from src.models.layers.monarch_linear import MonarchLinear\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([16, 1024])\n",
      "monarch factor shape (nblocks, block rank, block size):  torch.Size([4, 4, 256]) torch.Size([4, 256, 4])\n",
      "Total parameters: 0.01M,\n",
      "         trainable parameters: 0.01M (100.00%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MonarchLinear(in_features=1024, out_features=1024, nblocks=4, requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "from train_utils import param_stats, PEFT_CONFIG_PATH\n",
    "\n",
    "peft_config = json.load(open(PEFT_CONFIG_PATH, \"r\"))\n",
    "monarch = MonarchLinear(in_features=1024, out_features=1024, peft_config=peft_config)\n",
    "\n",
    "x = torch.randn(16, 1024, device=\"cuda\")\n",
    "print(\"out.shape:\", monarch(x).shape)\n",
    "print(\"monarch factor shape (nblocks, block rank, block size): \", monarch.blkdiag1.shape, monarch.blkdiag2.shape)\n",
    "param_stats(monarch)\n",
    "monarch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting layer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of exact layers to adapt with Monarch:  ['query', 'value', 'key']\n",
      "Can also modify the below options:\n",
      "adapt querys and keys only:  False\n",
      "adapt mlp: False\n",
      "making block rank = block size: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'monarch': True,\n",
       " 'square': False,\n",
       " 'nblocks': 4,\n",
       " 'blk_r': 4,\n",
       " 'blk_sz': None,\n",
       " 'layers_to_adapt': ['query', 'value', 'key'],\n",
       " 'q_v': False,\n",
       " 'adapter': True,\n",
       " 'scaler': False,\n",
       " 'layernorm': True,\n",
       " 'large_lr': False,\n",
       " 'new_lr': 0.005,\n",
       " 'scaler_type': 'scaler',\n",
       " 'from_lora': '',\n",
       " 'mlp': False,\n",
       " 'lora_style_init': False,\n",
       " 'use_mult_factor': False,\n",
       " 'affine': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Names of exact layers to adapt with Monarch: \", peft_config[\"layers_to_adapt\"])\n",
    "print(\"Can also modify the below options:\")\n",
    "print(\"adapt querys and keys only: \", peft_config[\"q_v\"])\n",
    "print(\"adapt mlp:\", peft_config[\"mlp\"])\n",
    "print(\"making block rank = block size:\", peft_config[\"square\"])\n",
    "# Can safely ignore other settings\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monarch factor shape: torch.Size([2, 8, 512]) torch.Size([2, 512, 8])\n"
     ]
    }
   ],
   "source": [
    "peft_config[\"blk_r\"] = 8\n",
    "peft_config[\"blk_sz\"] = 512\n",
    "monarch = MonarchLinear(in_features=1024, out_features=1024, peft_config=peft_config)\n",
    "print(\"monarch factor shape:\", monarch.blkdiag1.shape, monarch.blkdiag2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense matrix approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project to two monarch factors using SVD.\n",
    "# We don't use it for our setup, but if curious check blockdiag_butterfly_project_einsum_rank\n",
    "# from torch.testing import assert_allclose\n",
    "# from copy import deepcopy\n",
    "# m, n = 1024, 512\n",
    "# weights = torch.randn(m, n, device=\"cuda\")\n",
    "# monarch = MonarchLinear(in_features=n, out_features=m, weights=weights, peft_config=peft_config)\n",
    "# x = torch.eye(n, device=\"cuda\")\n",
    "# assert_allclose(monarch(x),  x @ weights.T )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora-style model adaptation for (theoretically) any model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 386.39M,\n",
      "         trainable parameters: 386.39M (100.00%)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    "    AutoModel\n",
    ")\n",
    "# model_name = \"microsoft/deberta-large\"\n",
    "model_name = \"meta-llama/Llama-2-7b\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "param_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up for the specific layer names to adapt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaModel(\n",
       "  (embeddings): DebertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=0)\n",
       "    (LayerNorm): DebertaLayerNorm()\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (encoder): DebertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x DebertaLayer(\n",
       "        (attention): DebertaAttention(\n",
       "          (self): DisentangledSelfAttention(\n",
       "            (in_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "            (pos_dropout): StableDropout()\n",
       "            (pos_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "            (pos_q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "          (output): DebertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): DebertaLayerNorm()\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "        (intermediate): DebertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): DebertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): DebertaLayerNorm()\n",
       "          (dropout): StableDropout()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (rel_embeddings): Embedding(1024, 1024)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapted in_proj (3072, 1024) with monarch layers: torch.Size([4, 4, 256]), torch.Size([4, 768, 4])\n",
      "Total parameters: 386.77M,\n",
      "         trainable parameters: 0.38M (0.10%)\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.deberta.modeling_deberta import DebertaAttention, DebertaIntermediate\n",
    "from train_utils import init_monarch_layers\n",
    "\n",
    "peft_config = json.load(open(PEFT_CONFIG_PATH, \"r\"))\n",
    "peft_config['layers_to_adapt'] = [\"in_proj\"]\n",
    "init_monarch_layers(model, peft_config, target_classes=[DebertaAttention, DebertaIntermediate])\n",
    "param_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wanna see what layers are adapted? 🥹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.0.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.0.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.1.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.1.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.2.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.2.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.3.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.3.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.4.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.4.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.5.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.5.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.6.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.6.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.7.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.7.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.8.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.8.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.9.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.9.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.10.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.10.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.11.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.11.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.12.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.12.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.13.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.13.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.14.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.14.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.15.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.15.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.16.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.16.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.17.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.17.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.18.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.18.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.19.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.19.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.20.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.20.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.21.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.21.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.22.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.22.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "encoder.layer.23.attention.self.in_proj.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.23.attention.self.in_proj.blkdiag2 : 0.0117M, torch.Size([4, 768, 4])\n",
      "Total parameters: 386.77M,\n",
      "         trainable parameters: 0.38M (0.10%)\n"
     ]
    }
   ],
   "source": [
    "param_stats(model, print_trainable=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
