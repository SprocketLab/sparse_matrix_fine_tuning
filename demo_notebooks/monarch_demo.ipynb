{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/wenxuan/miniconda3/envs/ldm/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# You might wanna connect to docker env using the command below inside docker, \n",
    "# then select jupyter server http://127.0.0.1:5050/lab?token=  (default password is 'local')\n",
    "# python3 -m jupyterlab --no-browser --ip=0.0.0.0 --port=5050 --allow-root --NotebookApp.token='local' --NotebookApp.password='local'\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(\"../\")\n",
    "os.chdir(\"../\")\n",
    "import torch\n",
    "from src.models.layers.monarch_linear import MonarchLinear\n",
    "import bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out.shape: torch.Size([16, 1024])\n",
      "monarch factor shape (nblocks, block rank, block size):  torch.Size([4, 4, 256]) torch.Size([4, 256, 4])\n",
      "Total parameters: 0.009M,\n",
      "         trainable parameters: 0.009M (100.000%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MonarchLinear(in_features=1024, out_features=1024, nblocks=4, requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "from train_utils import param_stats\n",
    "\n",
    "peft_config = json.load(open(\"task_configs/roberta_glue/peft_config.json\", \"r\"))\n",
    "monarch = MonarchLinear(in_features=1024, out_features=1024, peft_config=peft_config, as_adapter=False)\n",
    "\n",
    "x = torch.randn(16, 1024, device=\"cuda\")\n",
    "print(\"out.shape:\", monarch(x).shape)\n",
    "print(\"monarch factor shape (nblocks, block rank, block size): \", monarch.blkdiag1.shape, monarch.blkdiag2.shape)\n",
    "param_stats(monarch)\n",
    "monarch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting layer config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of exact layers to adapt with Monarch:  ['query', 'value', 'key']\n",
      "Can also modify the below options:\n",
      "adapt querys and keys only:  False\n",
      "adapt mlp: False\n",
      "making block rank = block size: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'monarch': True,\n",
       " 'square': False,\n",
       " 'nblocks': 4,\n",
       " 'blk_r': 4,\n",
       " 'blk_sz': None,\n",
       " 'layers_to_adapt': ['query', 'value', 'key'],\n",
       " 'q_v': False,\n",
       " 'adapter': True,\n",
       " 'scaler': False,\n",
       " 'layernorm': True,\n",
       " 'large_lr': False,\n",
       " 'new_lr': 0.005,\n",
       " 'scaler_type': 'scaler',\n",
       " 'from_lora': '',\n",
       " 'mlp': False,\n",
       " 'lora_style_init': False,\n",
       " 'use_mult_factor': False,\n",
       " 'affine': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Names of exact layers to adapt with Monarch: \", peft_config[\"layers_to_adapt\"])\n",
    "print(\"Can also modify the below options:\")\n",
    "print(\"adapt querys and keys only: \", peft_config[\"q_v\"])\n",
    "print(\"adapt mlp:\", peft_config[\"mlp\"])\n",
    "print(\"making block rank = block size:\", peft_config[\"square\"])\n",
    "# Can safely ignore other settings\n",
    "peft_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monarch factor shape: torch.Size([2, 8, 512]) torch.Size([2, 512, 8])\n"
     ]
    }
   ],
   "source": [
    "peft_config[\"blk_r\"] = 8\n",
    "peft_config[\"blk_sz\"] = 512\n",
    "monarch = MonarchLinear(in_features=1024, out_features=1024, peft_config=peft_config)\n",
    "print(\"monarch factor shape:\", monarch.blkdiag1.shape, monarch.blkdiag2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense matrix approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2072, -0.5451, -0.5087,  ...,  1.7620, -0.2555,  1.6147],\n",
       "         [ 1.1842,  0.5658,  0.7478,  ...,  1.4341,  0.3450,  2.3831],\n",
       "         [ 0.0557, -1.2869,  1.5041,  ...,  1.0188,  0.0603,  0.2268],\n",
       "         ...,\n",
       "         [-0.7492,  0.5547, -1.5522,  ..., -0.7151,  0.5899, -1.0649],\n",
       "         [ 3.0028, -0.8860, -1.1527,  ..., -1.1671,  1.2047, -0.7684],\n",
       "         [ 0.2830,  0.6385, -0.0437,  ...,  1.3922, -1.7917, -1.1091]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 0.2072,  1.0174,  0.0955,  ..., -0.5979,  2.8725,  0.3590],\n",
       "         [-0.3782,  0.5658, -1.3578,  ...,  0.6550, -0.9059,  0.5369],\n",
       "         [-0.5486,  0.8186,  1.5042,  ..., -1.3950, -1.2826, -0.3231],\n",
       "         ...,\n",
       "         [ 1.6108,  1.3338,  0.8617,  ..., -0.7152, -1.2136,  1.4127],\n",
       "         [-0.1251,  0.3648,  0.1901,  ...,  0.6362,  1.2047, -1.8486],\n",
       "         [ 1.5389,  2.4849,  0.5061,  ..., -1.0854, -0.7117, -1.1092]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project to two monarch factors using SVD.\n",
    "from src.ops.blockdiag_butterfly_einsum import blockdiag_butterfly_project_einsum_rank\n",
    "from src.models.layers.blockdiag_butterfly_multiply import blockdiag_butterfly_multiply\n",
    "weights = torch.randn(1024, 1024, device=\"cuda\")\n",
    "nblocks = 4\n",
    "rank = 1\n",
    "i = torch.eye(1024, device=\"cuda\")\n",
    "blkdiag1, blkdiag2, rev1, rev2 = blockdiag_butterfly_project_einsum_rank(weights, nblocks, nblocks, rank, reverse=True)\n",
    "# rev1, rev2 = blockdiag_butterfly_project_einsum_rank(weights, nblocks, nblocks, rank, reverse=True)\n",
    "(weights - blockdiag_butterfly_multiply(i, blkdiag1, blkdiag2)), blockdiag_butterfly_multiply(i, rev1, rev2)\n",
    "# We don't use it for our setup, but if curious check blockdiag_butterfly_project_einsum_rank\n",
    "# from torch.testing import assert_allclose\n",
    "# from copy import deepcopy\n",
    "# m, n = 1024, 512\n",
    "# weights = torch.randn(m, n, device=\"cuda\")\n",
    "# monarch = MonarchLinear(in_features=n, out_features=m, weights=weights, peft_config=peft_config)\n",
    "# x = torch.eye(n, device=\"cuda\")\n",
    "# assert_allclose(monarch(x),  x @ weights.T )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora-style model adaptation for (theoretically) any model from Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 338.897M,\n",
      "         trainable parameters: 338.897M (100.000%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "355359744"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    set_seed,\n",
    "    AutoModel\n",
    ")\n",
    "model_name = \"roberta-large\"\n",
    "# model_name = \"meta-llama/Llama-2-7b\"  # This one requies api key\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "param_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look up for the specific layer names to adapt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 1024)\n",
       "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-23): 24 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapted key (1024, 1024) with monarch layers: torch.Size([4, 4, 256]), torch.Size([4, 256, 4])\n",
      "Adapted value (1024, 1024) with monarch layers: torch.Size([4, 4, 256]), torch.Size([4, 256, 4])\n",
      "Adapted query (1024, 1024) with monarch layers: torch.Size([4, 4, 256]), torch.Size([4, 256, 4])\n",
      "Total parameters: 339.460M,\n",
      "         trainable parameters: 0.633M (0.186%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "663552"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_utils import init_monarch_layers\n",
    "\n",
    "peft_config = json.load(open(\"task_configs/roberta_glue/peft_config.json\", \"r\"))\n",
    "peft_config['layers_to_adapt'] = [\"query\", \"key\", \"value\"]\n",
    "init_monarch_layers(model, peft_config)\n",
    "param_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wanna see what layers are adapted? 🥹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layer.0.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.0.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.0.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.0.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.0.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.0.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.0.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.0.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.0.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.1.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.1.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.1.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.1.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.1.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.1.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.1.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.1.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.1.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.2.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.2.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.2.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.2.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.2.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.2.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.2.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.2.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.2.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.3.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.3.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.3.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.3.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.3.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.3.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.3.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.3.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.3.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.4.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.4.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.4.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.4.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.4.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.4.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.4.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.4.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.4.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.5.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.5.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.5.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.5.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.5.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.5.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.5.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.5.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.5.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.6.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.6.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.6.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.6.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.6.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.6.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.6.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.6.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.6.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.7.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.7.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.7.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.7.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.7.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.7.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.7.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.7.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.7.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.8.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.8.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.8.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.8.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.8.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.8.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.8.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.8.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.8.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.9.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.9.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.9.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.9.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.9.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.9.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.9.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.9.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.9.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.10.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.10.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.10.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.10.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.10.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.10.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.10.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.10.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.10.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.11.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.11.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.11.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.11.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.11.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.11.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.11.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.11.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.11.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.12.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.12.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.12.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.12.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.12.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.12.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.12.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.12.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.12.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.13.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.13.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.13.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.13.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.13.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.13.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.13.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.13.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.13.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.14.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.14.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.14.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.14.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.14.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.14.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.14.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.14.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.14.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.15.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.15.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.15.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.15.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.15.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.15.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.15.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.15.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.15.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.16.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.16.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.16.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.16.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.16.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.16.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.16.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.16.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.16.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.17.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.17.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.17.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.17.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.17.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.17.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.17.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.17.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.17.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.18.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.18.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.18.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.18.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.18.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.18.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.18.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.18.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.18.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.19.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.19.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.19.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.19.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.19.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.19.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.19.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.19.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.19.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.20.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.20.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.20.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.20.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.20.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.20.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.20.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.20.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.20.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.21.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.21.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.21.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.21.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.21.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.21.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.21.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.21.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.21.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.22.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.22.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.22.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.22.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.22.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.22.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.22.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.22.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.22.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.23.attention.self.query.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.23.attention.self.query.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.23.attention.self.query.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.23.attention.self.key.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.23.attention.self.key.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.23.attention.self.key.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "encoder.layer.23.attention.self.value.bias : 0.0010M, torch.Size([1024])\n",
      "encoder.layer.23.attention.self.value.blkdiag1 : 0.0039M, torch.Size([4, 4, 256])\n",
      "encoder.layer.23.attention.self.value.blkdiag2 : 0.0039M, torch.Size([4, 256, 4])\n",
      "Total parameters: 339.460M,\n",
      "         trainable parameters: 0.633M (0.186%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "663552"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_stats(model, print_trainable=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
