{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a49edf-f934-4223-b185-e3d29b6e0813",
   "metadata": {},
   "source": [
    "### Inference Speed Overhead Analysis of ReFT with pyvene\n",
    "\n",
    "When deployed in practice, ReFTs need to avoid overhead at inference time since any ReFT relies on interventions hooked into model's computation graph. Lucikily, through-out our experiments, we found that intervening on user prompt tokens are already enough to produce good performance on most of the tasks. As a result, the intervened representations or key-value pairs can be cached, and used for future decoding.\n",
    "\n",
    "In this tutorial, we try to compare the inferecen speed overhead of LoReFT with a model forward run without any intervention (i.e., the ceiling runtime). In theory, ReFTs runtime should be:\n",
    "\n",
    "- **worse than LoRA**, since LoRA can merge its learned weights into the model weights, resulting in zero-overhead at inference-time.\n",
    "- **better than Adaptor**, since Adaptor applies additional computes to every steps in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9ccc0-30b7-42c6-8424-27a0302f0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time, json\n",
    "import transformers\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "from pyreft import (\n",
    "    TaskType,\n",
    "    get_reft_model,\n",
    "    ReftConfig,\n",
    "    ReftTrainerForCausalLM, \n",
    "    ReftDataCollator,\n",
    "    ReftSupervisedDataset,\n",
    "    LoreftIntervention\n",
    ")\n",
    "\n",
    "prompt_no_input_template = \"\"\"Below is an instruction that \\\n",
    "describes a task. Write a response that appropriately \\\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "raw_dataset = load_dataset(\"json\", data_files=\"../composition/ultrafeedback_1k.json\")[\"train\"].select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1360cd6a-c267-4576-8dd6-5be736502255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model (take 1 min)\n",
    "model_name_or_path = \"yahma/llama-7b-hf\" # yahma/llama-7b-hf or yahma/llama-13b-hf\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "# get tokenizer\n",
    "model_max_length = 1024\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=model_max_length, \n",
    "    padding_side=\"right\", use_fast=False, truncate=True)\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bfd64-07f6-49b8-b742-3c9b938285f8",
   "metadata": {},
   "source": [
    "### Rank analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454a396-a1c4-4762-b4b0-99e069636813",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANKS = [1,4,8,16,32]\n",
    "TARGET_LAYER = 15\n",
    "\n",
    "elapse_per_rank = {}\n",
    "for RANK in RANKS:\n",
    "    print(\"analyzing:\", RANK)\n",
    "    # get reft model\n",
    "    reft_config = ReftConfig(representations={\n",
    "        \"layer\": TARGET_LAYER, \"component\": \"block_output\",\n",
    "        \"intervention\": LoreftIntervention(\n",
    "        embed_dim=model.config.hidden_size, low_rank_dimension=RANK)})\n",
    "    reft_model = get_reft_model(model, reft_config)\n",
    "    reft_model.print_trainable_parameters()\n",
    "\n",
    "    all_elapse = []\n",
    "    for example in raw_dataset:\n",
    "        instruction = example[\"instruction\"]\n",
    "        \n",
    "        prompt = prompt_no_input_template % instruction\n",
    "        prompt = tokenizer(prompt, max_length=model_max_length, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        base_unit_location = prompt[\"input_ids\"].shape[-1] - 1  # last position\n",
    "\n",
    "        start = time.time()\n",
    "        # loreft generate\n",
    "        _, reft_response = reft_model.generate(\n",
    "            prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "            intervene_on_prompt=True, max_new_tokens=256, do_sample=False, \n",
    "        )\n",
    "        end = time.time()\n",
    "        elapse = end - start\n",
    "        prompt_len = prompt['input_ids'].shape[-1]\n",
    "\n",
    "        all_len = len(reft_response[0])\n",
    "        all_elapse += [(elapse, prompt_len, all_len, \"loreft\")]\n",
    "\n",
    "        start = time.time()\n",
    "        # vanilla generate\n",
    "        model_response = model.generate(\n",
    "            **prompt, max_new_tokens=256, do_sample=False, \n",
    "        )\n",
    "        end = time.time()\n",
    "        elapse = end - start\n",
    "        all_len = len(model_response[0])\n",
    "        all_elapse += [(elapse, prompt_len, all_len, \"vanilla\")]\n",
    "        \n",
    "    elapse_per_rank[RANK] = all_elapse\n",
    "with open(\"../plots/data/elapse_per_rank.json\", 'w') as f:\n",
    "    json.dump(elapse_per_rank, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cf28a2-1651-47d5-aadb-b35998d21fd5",
   "metadata": {},
   "source": [
    "### Layer analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6403cda-571d-43e0-b56a-ad7f3791fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [[12,13],[12,13,14,15],[12,13,14,15,16,17],[12,13,14,15,16,17,18,19],[12,13,14,15,16,17,18,19,20,21]]\n",
    "RANK = 8\n",
    "\n",
    "elapse_per_layer = {}\n",
    "for LAYER in LAYERS:\n",
    "    print(\"analyzing:\", LAYER)\n",
    "    # get reft model\n",
    "    reft_config = ReftConfig(representations=[{\n",
    "        \"layer\": l, \"component\": \"block_output\",\n",
    "        \"intervention\": LoreftIntervention(\n",
    "        embed_dim=model.config.hidden_size, low_rank_dimension=RANK)} for l in LAYER])\n",
    "    reft_model = get_reft_model(model, reft_config)\n",
    "    reft_model.print_trainable_parameters()\n",
    "\n",
    "    all_elapse = []\n",
    "    for example in raw_dataset:\n",
    "        instruction = example[\"instruction\"]\n",
    "        \n",
    "        prompt = prompt_no_input_template % instruction\n",
    "        prompt = tokenizer(prompt, max_length=model_max_length, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        base_unit_location = prompt[\"input_ids\"].shape[-1] - 1  # last position\n",
    "\n",
    "        start = time.time()\n",
    "        # loreft generate\n",
    "        _, reft_response = reft_model.generate(\n",
    "            prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]]*len(LAYER))},\n",
    "            intervene_on_prompt=True, max_new_tokens=256, do_sample=False, \n",
    "        )\n",
    "        end = time.time()\n",
    "        elapse = end - start\n",
    "        prompt_len = prompt['input_ids'].shape[-1]\n",
    "        all_len = len(reft_response[0])\n",
    "        all_elapse += [(elapse, prompt_len, all_len, \"loreft\")]\n",
    "\n",
    "        start = time.time()\n",
    "        # vanilla generate\n",
    "        model_response = model.generate(\n",
    "            **prompt, max_new_tokens=256, do_sample=False, \n",
    "        )\n",
    "        end = time.time()\n",
    "        elapse = end - start\n",
    "        all_len = len(model_response[0])\n",
    "        all_elapse += [(elapse, prompt_len, all_len, \"vanilla\")]\n",
    "        \n",
    "    elapse_per_layer[\";\".join([str(l) for l in LAYER])] = all_elapse\n",
    "with open(\"../plots/data/elapse_per_layer.json\", 'w') as f:\n",
    "    json.dump(elapse_per_layer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9ef7a-f2ae-4ac5-a50c-b918bf79eb0e",
   "metadata": {},
   "source": [
    "### Position analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7091f75-314b-45ec-a59d-bed86002fcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = [2, 4, 6, 8, 10]\n",
    "RANK = 8\n",
    "LAYER = 15\n",
    "\n",
    "elapse_per_position = {}\n",
    "for position in positions:\n",
    "    print(\"analyzing:\", position)\n",
    "    # get reft model\n",
    "    reft_config = ReftConfig(representations=[{\n",
    "        \"layer\": LAYER, \"component\": \"block_output\",\n",
    "        \"intervention\": LoreftIntervention(\n",
    "        embed_dim=model.config.hidden_size, low_rank_dimension=RANK)}])\n",
    "    reft_model = get_reft_model(model, reft_config)\n",
    "    reft_model.print_trainable_parameters()\n",
    "\n",
    "    all_elapse = []\n",
    "    for example in raw_dataset:\n",
    "        instruction = example[\"instruction\"]\n",
    "        \n",
    "        prompt = prompt_no_input_template % instruction\n",
    "        prompt = tokenizer(prompt, max_length=model_max_length, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        base_unit_location = prompt[\"input_ids\"].shape[-1] - 1  # last position\n",
    "\n",
    "        start = time.time()\n",
    "        # loreft generate\n",
    "        _, reft_response = reft_model.generate(\n",
    "            prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location-i for i in range(position)]]])},\n",
    "            intervene_on_prompt=True, max_new_tokens=256, do_sample=False, \n",
    "        )\n",
    "        end = time.time()\n",
    "        elapse = end - start\n",
    "        prompt_len = prompt['input_ids'].shape[-1]\n",
    "        all_len = len(reft_response[0])\n",
    "        all_elapse += [(elapse, prompt_len, all_len, \"loreft\")]\n",
    "        \n",
    "        start = time.time()\n",
    "        # vanilla generate\n",
    "        model_response = model.generate(\n",
    "            **prompt, max_new_tokens=256, do_sample=False, \n",
    "        )\n",
    "        end = time.time()\n",
    "        elapse = end - start\n",
    "        all_len = len(model_response[0])\n",
    "        all_elapse += [(elapse, prompt_len, all_len, \"vanilla\")]\n",
    "        \n",
    "    elapse_per_position[position] = all_elapse\n",
    "with open(\"../plots/data/elapse_per_position.json\", 'w') as f:\n",
    "    json.dump(elapse_per_position, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
