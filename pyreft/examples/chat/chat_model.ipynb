{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515fcf7a-c011-4a5f-9f78-b58feffaeb26",
   "metadata": {},
   "source": [
    "### Loading base LM and our 18min-cooked ReFT model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222060d4-bb6e-4edc-8119-86a4f7d85b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d90a03ce3e48788498d1848584df26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea191918c5d4f5aa3e0c0aef761a9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The key is provided in the config. Assuming this is loaded from a pretrained module.\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers\n",
    "from pyreft import (\n",
    "    ReftModel,\n",
    "    get_intervention_locations\n",
    ")\n",
    "\n",
    "prompt_no_input_template = \"\"\"Below is an instruction that \\\n",
    "describes a task. Write a response that appropriately \\\n",
    "completes the request.\n",
    "\n",
    "### Instruction:\n",
    "%s\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "reft_model_name_or_path = \"zhengxuanzenwu/Loreft1k-Llama-2-7b-hf\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, model_max_length=2048, padding_side=\"right\", use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "reft_model = ReftModel.load(\n",
    "    reft_model_name_or_path, model, from_huggingface_hub=True)\n",
    "reft_model.set_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5eb8d-95d6-4a53-a3dd-0575e0ee0efd",
   "metadata": {},
   "source": [
    "### Inference with intervened locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a45798a-ae5f-41ce-8cc1-9fca794a49bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can you give any tips on how to cook a juicy, medium-rare steak?\n",
      "\n",
      "### Response:\n",
      "Sure! Here are some tips for cooking a juicy, delicious medium-rare steaks:\n",
      "1. Choose high-quality beef with a good marbling of fat. This will ensure that your steak stays moist and tender while cooking.\n",
      "2. Season your steak generously with salt and pepper before grilling or pan-frying it. This will help enhance its flavor.\n",
      "3. Cook your steak over medium heat until it reaches an internal temperature of 140°F (60°C). This will ensure that the meat remains pink inside without overcooking it.\n",
      "4. Let your steak rest for at least five minutes after removing it from the heat source so that all of the juices can redistribute throughout the meat.\n",
      "5. Serve your steak with a side dish like mashed potatoes or sautéed vegetables for added flavor and texture contrast.\n",
      "\n",
      "I hope these tips help you create a mouthwatering meal!\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Can you give any tips on how to cook a juicy, medium-rare steak?\"\n",
    "\n",
    "# tokenize and prepare the input\n",
    "prompt = prompt_no_input_template % instruction\n",
    "prompt = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "intervention_locations = torch.tensor([get_intervention_locations(\n",
    "    last_position=prompt[\"input_ids\"].shape[-1], positions=\"f5+l5\",\n",
    "    num_interventions=len(reft_model.interventions))]).permute(1, 0, 2).tolist()\n",
    "\n",
    "# generate\n",
    "_, reft_response = reft_model.generate(\n",
    "    prompt, \n",
    "    unit_locations={\"sources->base\": (None, intervention_locations)},\n",
    "    intervene_on_prompt=True, max_new_tokens=512, do_sample=False, \n",
    "    no_repeat_ngram_size=5, repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(reft_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b34e4-9374-4508-82ac-7edef66bdd73",
   "metadata": {},
   "source": [
    "### Llama-2 7B base LM can follow certain instructions already!\n",
    "\n",
    "We follow previous works to evaluate with Llama-2; but you should try on other base LMs as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b069a40-299f-412a-8071-3900539ea1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Can you give any tips on how to cook a juicy, medium-rare steak?\n",
      "\n",
      "### Response:\n",
      "I've been cooking steaks for years and I have found that the best way to get a juicy, rare steak is to use a cast iron skillet. The cast iron skillet will help to sear the outside of the steak while keeping it moist on the inside. You should also make sure to season your cast iron skillet before using it so that it doesn't stick to the meat when cooking.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "model_response = model.generate(\n",
    "    **prompt, \n",
    "    max_new_tokens=512, do_sample=False, \n",
    "    no_repeat_ngram_size=5, repetition_penalty=1.1,\n",
    "    eos_token_id=tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(tokenizer.decode(model_response[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
