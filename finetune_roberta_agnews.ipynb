{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d8a16e-e083-498c-a301-426330554674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total GPU memory: 39.430908 GB\n",
      "available GPU memory 38.603821 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n",
    "from transformers import (\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModel,\n",
    "    AutoConfig,\n",
    ")\n",
    "print(\"total GPU memory: %f GB\" % (torch.cuda.mem_get_info()[1] / 1024 ** 3))\n",
    "print(\"available GPU memory %f GB\" % (torch.cuda.mem_get_info()[0] / 1024 ** 3))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cpu\":\n",
    "    logging.warning(\"No GPU available, using CPU will be super slow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2573b9-72b9-4267-91a5-2e9dcefb5d76",
   "metadata": {},
   "source": [
    "### Load dataset, model, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f68e2ea-76b9-4059-b809-c7d643692e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"ag_news\"\n",
    "model_id = \"roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94d0242e-fc9c-4445-9902-f115f688a004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset ag_news (/nobackup/wenxuan/huggingface_cache/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e34c457e3f42f3affc2b97ab82d2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c04fabca-7e70-4e7e-bec8-e278e844b1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset[\"test\"].shard(num_shards=2, index=0)\n",
    "val_dataset = dataset[\"test\"].shard(num_shards=2, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e008657e-0618-45ac-8c2f-83a6b33214fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3928576f3846d28597482bae4a5ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa11ebb3f45e409eae60873760b80bfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fa933777c041689615084208661a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "\n",
    "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b83a821-e392-4160-8555-4c500edc028e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels: 4\n",
      "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "num_labels = dataset['train'].features['label'].num_classes\n",
    "class_names = dataset[\"train\"].features[\"label\"].names\n",
    "\n",
    "print(f\"number of labels: {num_labels}\")\n",
    "print(f\"the labels: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807fbe7-5dc4-4ba3-b65a-47de37aa56ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"roberta-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"World\",\n",
       "    \"1\": \"Sports\",\n",
       "    \"2\": \"Business\",\n",
       "    \"3\": \"Sci/Tech\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.33.2\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {i: label for i, label in enumerate(class_names)}\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.update({\"id2label\": id2label})\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56bdcf-b08e-4ca0-b600-e068fa0dbaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f5ae44e42e0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "roberta_model = RobertaForSequenceClassification.from_pretrained(model_id, config=config).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a2748-204b-4c34-9f92-5d6cc5ebf52f",
   "metadata": {},
   "source": [
    "### Evaluate performance before fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be25b09e-725f-42c9-b096-69e8700aa46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3800/3800 [00:27<00:00, 135.85it/s]\n"
     ]
    }
   ],
   "source": [
    "roberta_predictions = []\n",
    "for i in tqdm(range(len(test_dataset[\"text\"])), total=len(test_dataset[\"text\"])):\n",
    "    test_input = tokenizer(test_dataset[\"text\"][i], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = roberta_model(**test_input).logits\n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    roberta_predictions.append(predicted_class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50cb4f-dad0-49ce-b447-b1e1ab0baf61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw roberta accuracy:  0.249\n"
     ]
    }
   ],
   "source": [
    "print(\"raw roberta accuracy: \", round(accuracy_score(test_dataset[\"label\"], roberta_predictions), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f853f4-175c-4078-96f8-49422be48740",
   "metadata": {},
   "source": [
    "### Fine-tune Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c491439-ae71-4dd8-a3e1-2deb76358563",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/fine_tuned_agnews\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=180,\n",
    "    per_device_eval_batch_size=380,\n",
    "    learning_rate=0.005,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    warmup_steps=50,\n",
    "    eval_steps=100,\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a7a73-0d16-43ab-9338-ee3449498b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bdbcab-9766-4986-a450-638cef6dec83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='667' max='667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [667/667 07:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.386237</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.autocast(device, cache_enabled=False):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f048aec-4b59-4709-b686-3cf5a439816b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1/10 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3863519430160522,\n",
       " 'eval_runtime': 12.4551,\n",
       " 'eval_samples_per_second': 305.095,\n",
       " 'eval_steps_per_second': 0.803,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9060c-2976-43e0-852e-b9fb4ab6398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_roberta_outputs = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff7fc5-968e-43f7-8104-0802082ef9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_roberta_predictions = finetuned_roberta_outputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487818c4-daa5-4148-b00f-bd641ab5e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine-tuned roberta accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"fine-tuned roberta accuracy: \", round(accuracy_score(test_dataset[\"label\"], finetuned_roberta_predictions), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0efa276-2f44-4edf-95ec-18ace5a9d42b",
   "metadata": {},
   "source": [
    "### Load fine-tuned Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a3a2df-abe7-4489-b900-b2c6b1928e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = RobertaForSequenceClassification.from_pretrained(\"./results/fine_tuned_agnews/checkpoint-667/\", local_files_only=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
