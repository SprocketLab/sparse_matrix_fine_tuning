{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1709388724.560207] [80c05f596a00:21676:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n",
      "[2024-03-02 14:12:18,472] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# python3 -m jupyterlab --no-browser --ip=0.0.0.0 --port=5050 --allow-root --NotebookApp.token='local'\n",
    "import json\n",
    "import transformers\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from src.models.modeling_roberta import RobertaForSequenceClassification\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from train_utils import *\n",
    "\n",
    "# helper to init and set hyperparams for Ray Tune search\n",
    "def model_init(peft_config, hyperparams = None, use_monarch = True):\n",
    "    torch.manual_seed(42)\n",
    "    model_name_or_path = \"roberta-large\"\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name_or_path,\n",
    "        from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter search\n",
    "    if hyperparams is not None:\n",
    "        for k in peft_config.keys():\n",
    "            if k in hyperparams.keys():\n",
    "                print(\"Overriding {} with {}\".format(k, peft_config[k]))\n",
    "                peft_config[k] = hyperparams[k]\n",
    "\n",
    "    if use_monarch:\n",
    "        model.roberta.set_peft_config(peft_config)\n",
    "    # NOTE: Ray doesn't support torch.compile and it also causes a bug with trainer...\n",
    "    # if torch.__version__.startswith(\"2\") and not do_tune:\n",
    "    #     model = torch.compile(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_dict = torch.load(\"results/lora/cola/model/checkpoint-13547/pytorch_model.bin\")\n",
    "# for name, module in lora_dict.items():\n",
    "#     if \"lora\" in name:\n",
    "#         print(name, \",\", module.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check layer-wise differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter checkpoints you want to compare\n",
    "ckpt_1 = \"main_init.pt\"\n",
    "ckpt_2 = \"train_init.pt\"\n",
    "ckpt_1 = torch.load(ckpt_1, map_location='cpu')\n",
    "ckpt_2 = torch.load(ckpt_2, map_location='cpu')\n",
    "for name, module in ckpt_1.items():\n",
    "    if not torch.allclose(module, ckpt_2[name]):\n",
    "        print(name, \" is updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.0.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.0.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.0.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.0.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.0.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.0.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.1.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.1.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.1.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.1.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.1.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.1.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.2.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.2.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.2.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.2.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.2.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.2.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.3.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.3.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.3.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.3.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.3.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.3.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.4.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.4.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.4.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.4.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.4.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.4.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.5.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.5.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.5.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.5.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.5.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.5.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.6.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.6.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.6.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.6.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.6.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.6.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.7.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.7.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.7.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.7.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.7.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.7.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.8.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.8.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.8.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.8.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.8.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.8.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.9.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.9.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.9.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.9.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.9.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.9.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.10.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.10.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.10.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.10.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.10.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.10.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.11.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.11.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.11.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.11.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.11.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.11.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.12.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.12.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.12.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.12.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.12.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.12.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.13.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.13.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.13.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.13.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.13.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.13.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.14.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.14.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.14.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.14.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.14.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.14.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.15.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.15.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.15.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.15.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.15.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.15.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.16.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.16.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.16.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.16.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.16.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.16.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.17.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.17.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.17.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.17.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.17.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.17.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.18.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.18.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.18.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.18.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.18.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.18.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.19.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.19.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.19.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.19.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.19.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.19.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.20.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.20.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.20.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.20.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.20.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.20.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.21.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.21.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.21.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.21.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.21.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.21.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.22.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.22.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.22.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.22.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.22.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.22.attention.self.value.blkdiag1  is updated\n",
      "roberta.encoder.layer.23.attention.self.query.bias  is updated\n",
      "roberta.encoder.layer.23.attention.self.query.blkdiag1  is updated\n",
      "roberta.encoder.layer.23.attention.self.key.bias  is updated\n",
      "roberta.encoder.layer.23.attention.self.key.blkdiag1  is updated\n",
      "roberta.encoder.layer.23.attention.self.value.bias  is updated\n",
      "roberta.encoder.layer.23.attention.self.value.blkdiag1  is updated\n",
      "classifier.dense.weight  is updated\n",
      "classifier.dense.bias  is updated\n",
      "classifier.out_proj.weight  is updated\n",
      "classifier.out_proj.bias  is updated\n"
     ]
    }
   ],
   "source": [
    "peft_config = json.load(\"task_configs/glue_peft_configs/peft_monarch.json\")\n",
    "model = model_init(peft_config, use_monarch = True)\n",
    "init_dict = model.state_dict() # initial weights\n",
    "trained_model = torch.load\n",
    "\n",
    "for name, module in init_dict.items():\n",
    "\n",
    "    if not torch.allclose(module.cuda(), monarch_dict[name]):\n",
    "        print(name, \" is updated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test grad hook (ignore this please)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.4000], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.1200], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.0160], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.9088], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.6198], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0069], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9926], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.5798], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.8510], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.9520], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.0626], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.3607], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0133], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0153], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.4408], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.1782], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.0800], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.9657], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.6583], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0193], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9764], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.5382], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.7923], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.8881], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.0062], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.3230], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0247], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0325], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.4832], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.2373], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.1439], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.0217], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.6952], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0297], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9582], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.4951], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.7329], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.8242], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.9507], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.2870], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0341], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0516], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.5270], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.2970], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.2076], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.0767], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.7304], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0381], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9381], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.4505], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.6728], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.7605], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.8962], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.2526], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0415], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0726], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.5723], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.3575], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.2712], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.1307], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.7640], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0445], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9161], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.4045], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.6120], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.6971], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.8428], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.2199], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0470], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0955], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.6190], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.4186], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.3345], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.1835], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.7958], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0490], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.8923], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.3572], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.5507], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.6340], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.7905], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.1889], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0504], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.1203], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.6670], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.4802], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.3975], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.2352], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.8259], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0514], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.8667], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.3086], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.4887], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.5712], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.7394], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.1597], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0519], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.1469], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.7163], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def hook_fn(module, grad_input, grad_output):\n",
    "    print(f\"{module} has dW {grad_input[1]} and scaler value {module.scaler}\")\n",
    "    \n",
    "    \n",
    "def factor_balance(mid_blksz, out_blksz):\n",
    "    total = mid_blksz * out_blksz\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self, out_features):\n",
    "        super().__init__()\n",
    "        self.scaler = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.requires_grad_(True)\n",
    "        x = self.scaler * x\n",
    "        # layernorm to avoid vanishing gradient\n",
    "        return x\n",
    "\n",
    "x = torch.ones(100, 100, dtype=torch.float32)\n",
    "y = torch.full((100, 100), 2, dtype=torch.float32)\n",
    "model = Scaler(100)\n",
    "model.register_backward_hook(hook_fn)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for i in range(100):\n",
    "    loss = F.mse_loss(model(x), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
