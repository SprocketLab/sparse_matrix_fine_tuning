{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import transformers\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from fly_src.models.modeling_roberta import RobertaForSequenceClassification\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from train_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def hook_fn(module, grad_input, grad_output):\n",
    "    print(f\"{module} has dW {grad_input[1]} and scaler value {module.scaler}\")\n",
    "    \n",
    "    \n",
    "def factor_balance(mid_blksz, out_blksz):\n",
    "    total = mid_blksz * out_blksz\n",
    "\n",
    "# @Wenxuan\n",
    "# Use a diagonal matrix or a scaler to scale output of monarch factors\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self, out_features):\n",
    "        super().__init__()\n",
    "        self.scaler = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.scaler * x\n",
    "        x = F.layer_norm(x, x.shape[1:])\n",
    "        # layernorm to avoid vanishing gradient\n",
    "        return x\n",
    "\n",
    "x = torch.ones(100, 100)\n",
    "y = torch.full((100, 100), 2)\n",
    "model = Scaler(100)\n",
    "model.register_backward_hook(hook_fn)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for i in range(100):\n",
    "    loss = F.mse_loss(model(x), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
