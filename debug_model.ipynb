{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-29 16:22:00,577] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# python3 -m jupyterlab --no-browser --ip=0.0.0.0 --port=5050 --allow-root --NotebookApp.token='local'\n",
    "import json\n",
    "import transformers\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from fly_src.models.modeling_roberta import RobertaForSequenceClassification\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version\n",
    "from train_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.0.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.0.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.0.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.0.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.1.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.1.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.1.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.1.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.2.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.2.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.2.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.2.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.3.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.3.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.3.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.3.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.4.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.4.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.4.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.4.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.5.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.5.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.5.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.5.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.6.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.6.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.6.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.6.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.7.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.7.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.7.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.7.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.8.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.8.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.8.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.8.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.9.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.9.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.9.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.9.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.10.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.10.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.10.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.10.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.11.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.11.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.11.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.11.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.12.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.12.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.12.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.12.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.13.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.13.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.13.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.13.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.14.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.14.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.14.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.14.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.15.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.15.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.15.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.15.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.16.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.16.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.16.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.16.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.17.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.17.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.17.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.17.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.18.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.18.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.18.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.18.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.19.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.19.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.19.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.19.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.20.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.20.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.20.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.20.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.21.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.21.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.21.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.21.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.22.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.22.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.22.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.22.attention.self.value.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.23.attention.self.query.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.23.attention.self.query.lora_B , torch.Size([1024, 8])\n",
      "roberta.encoder.layer.23.attention.self.value.lora_A , torch.Size([8, 1024])\n",
      "roberta.encoder.layer.23.attention.self.value.lora_B , torch.Size([1024, 8])\n"
     ]
    }
   ],
   "source": [
    "lora_dict = torch.load(\"results/lora/cola/model/checkpoint-13547/pytorch_model.bin\")\n",
    "for name, module in lora_dict.items():\n",
    "    if \"lora\" in name:\n",
    "        print(name, \",\", module.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta.encoder.layer.0.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.0.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.1.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.1.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.2.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.2.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.3.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.3.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.4.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.4.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.5.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.5.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.6.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.6.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.7.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.7.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.8.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.8.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.9.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.9.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.10.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.10.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.11.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.11.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.12.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.12.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.13.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.13.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.14.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.14.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.15.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.15.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.16.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.16.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.17.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.17.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.18.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.18.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.19.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.19.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.20.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.20.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.21.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.21.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.22.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.22.attention.self.key.dense , torch.Size([1024, 1024])\n",
      "roberta.encoder.layer.23.attention.self.key.bias , torch.Size([1024])\n",
      "roberta.encoder.layer.23.attention.self.key.dense , torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "monarch_dict = torch.load(\"results/monarch_roberta_glue/cola/checkpoint-2250/pytorch_model.bin\")\n",
    "for name, module in monarch_dict.items():\n",
    "    if \"key\" in name:\n",
    "        print(name, \",\", module.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.4000], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.1200], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.0160], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.9088], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.6198], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0069], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9926], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.5798], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.8510], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.9520], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.0626], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.3607], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0133], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0153], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.4408], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.1782], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.0800], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.9657], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.6583], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0193], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9764], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.5382], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.7923], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.8881], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.0062], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.3230], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0247], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0325], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.4832], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.2373], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.1439], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.0217], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.6952], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0297], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9582], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.4951], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.7329], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.8242], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.9507], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.2870], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0341], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0516], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.5270], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.2970], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.2076], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.0767], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.7304], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0381], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9381], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.4505], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.6728], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.7605], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.8962], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.2526], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0415], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0726], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.5723], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.3575], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.2712], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.1307], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.7640], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0445], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.9161], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.4045], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.6120], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.6971], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.8428], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.2199], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0470], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.0955], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.6190], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.4186], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.3345], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.1835], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.7958], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0490], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.8923], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.3572], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.5507], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.6340], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.7905], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.1889], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0504], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.1203], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.6670], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.4802], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.3975], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.2352], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.8259], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([4.0514], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.8667], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([3.3086], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([2.4887], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([1.5712], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.7394], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.1597], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([-0.0519], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.1469], requires_grad=True)\n",
      "Scaler() has dW None and scaler value Parameter containing:\n",
      "tensor([0.7163], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "def hook_fn(module, grad_input, grad_output):\n",
    "    print(f\"{module} has dW {grad_input[1]} and scaler value {module.scaler}\")\n",
    "    \n",
    "    \n",
    "def factor_balance(mid_blksz, out_blksz):\n",
    "    total = mid_blksz * out_blksz\n",
    "\n",
    "class Scaler(nn.Module):\n",
    "    def __init__(self, out_features):\n",
    "        super().__init__()\n",
    "        self.scaler = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.requires_grad_(True)\n",
    "        x = self.scaler * x\n",
    "        # layernorm to avoid vanishing gradient\n",
    "        return x\n",
    "\n",
    "x = torch.ones(100, 100, dtype=torch.float32)\n",
    "y = torch.full((100, 100), 2, dtype=torch.float32)\n",
    "model = Scaler(100)\n",
    "model.register_backward_hook(hook_fn)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "for i in range(100):\n",
    "    loss = F.mse_loss(model(x), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
